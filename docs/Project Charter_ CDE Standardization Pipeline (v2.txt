Project Charter: CDE Standardization Pipeline (v2.0)
1. Guiding Principles & Goals
Primary Goal: To create a highly efficient, automated, and robust pipeline for standardizing a large catalog of Common Data Elements (CDEs), with a focus on improving data quality and identifying redundancy1.


Core Philosophy: Our approach is guided by four key principles that have shaped the implementation of every stage2:


* Efficiency: Every design choice must minimize token usage, monetary cost, and total runtime3. We favor intelligent pre-processing to reduce the burden on the AI model4.
* Robustness: The pipeline must be resilient to failures5. It is designed to be restartable, ensuring that an interruption does not require starting the entire process from scratch6.
* Simplicity & Modularity: The system is broken into simple, independent stages7. Each component has a single, well-defined responsibility, making it easy to develop, test, and maintain8.
* Intelligence: We aim to go beyond simple field edits9. The pipeline is designed to understand the nuanced relationships between CDEs, allowing it to correctly identify true duplicates while respecting necessary variations within a functional family of elements10.
2. The 4-Stage Pipeline Architecture
The project is implemented as a linear sequence of four distinct, check-pointed stages11. Stages 1 and 2 are complete, providing the foundation for the current work in Stage 3.


* Stage 1: Pre-processing & Filtering (Completed)
   * Input: The raw, complete CDE catalog12.
   * Implementation: This stage applies a set of quality heuristics to flag CDEs that are candidates for review (e.g., those with missing descriptions or invalid variable names)13. A key lesson learned was that explicit, auditable methods are superior to complex, inferred logic. Therefore, a dictionary-based mapping file was implemented to deterministically standardize the permissible_values field, a major source of initial data quality issues.
   * Output: The original CDE catalog, augmented with a needs_audit flag for downstream processing14.
* Stage 2: Graph-Based Community Detection (Completed)
   * Input: The full, flagged CDE catalog from Stage 115.
   * Implementation: This stage intelligently groups CDEs into contextually rich "communities" to enable high-level reasoning in Stage 316. To achieve this, a multi-faceted knowledge graph was built where each CDE is a node. Edges between nodes were weighted using a multiplicative boosting strategy that combines multiple facets of similarity17:
      1. Semantic Weight: Cosine similarity between text embeddings of key fields (title, short_description, etc.).
      2. Lexical Weight: Levenshtein distance or Jaccard similarity between variable_name fields to catch typos.
      3. Structural Weight: A score based on matching value_format and permissible_values formats.
   * A Louvain community detection algorithm was then run on the graph to identify dense clusters of related CDEs18. To respect the technical constraints of the AI model, any community larger than a set size (MAX_COMMUNITY_SIZE) was automatically subdivided using a "Hub-and-Spoke" model. This ensures no single input to the AI is too large, while maintaining context by including the "hub" CDE in each sub-group.
   * Output: A JSON file containing the final CDE "communities" to be sent for AI review19.
* Stage 3: AI-Powered Adjudication (Current Stage)
   * Input: The JSON file of CDE communities from Stage 220.
   * Mandate: The Stage 3 team is responsible for developing a parallelized Python script that sends each community to the Gemini API21. The prompt will instruct the model to perform detailed analysis, including filling empty fields, standardizing values, and flagging redundant CDEs22. The output must be a single, flat JSON array of suggestions for each CDE23. A critical component of this stage is the manifest file for tracking the status of each API call to ensure the process is fully restartable242424.
   * Output: A directory of raw JSON responses from the API, one for each successfully processed community25.
* Stage 4: Post-processing & Integration (Upcoming)
   * Input: The directory of raw JSON responses and the original CDE catalog26.
   * Process: A script will parse all JSON responses, using Pydantic models for strict validation27. The validated suggestions will then be merged back into the main CDE catalog28.
   * Output: The final, enriched CDE catalog as a CSV file29.
3. Key Implementation Best Practices
* State Management & Restartability: The use of checkpoint files (e.g., pickled graph objects in Stage 2) and a "manifest" file (in Stage 3) is critical for ensuring that long-running or expensive processes are idempotent and can recover gracefully from any interruption30.
* Structured I/O: All AI model interactions must request JSON mode, and all responses will be validated against Pydantic models to ensure data integrity and prevent errors in the final integration stage31.
* Configuration & Testing: The pipeline supports a --dry-run mode for rapid testing32. System prompts are version-controlled to ensure reproducibility33.
* Monitoring: The cost and performance of every API call will be logged to a CSV file, enabling detailed analysis and optimization34.